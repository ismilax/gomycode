# -*- coding: utf-8 -*-
"""Creating Your Own Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-sj9G61UQEs2sYaYtg6VjcsJj1DY_bD5
"""

import numpy as np
import json
import re
import tensorflow as tf
import random
import spacy
nlp = spacy.load('en_core_web_sm')

with open('/content/drive/MyDrive/intents.json') as f:
    intents = json.load(f)

def preprocessing(line):
    line = re.sub(r'[^a-zA-z.?!\']', ' ', line)
    line = re.sub(r'[ ]+', ' ', line)
    return line

# get text and intent title from json data
inputs, targets = [], []
classes = []
intent_doc = {}

for intent in intents['intents']:
    if intent['intent'] not in classes:
        classes.append(intent['intent'])
    if intent['intent'] not in intent_doc:
        intent_doc[intent['intent']] = []

    for text in intent['text']:
        inputs.append(preprocessing(text))
        targets.append(intent['intent'])

    for response in intent['responses']:
        intent_doc[intent['intent']].append(response)

def tokenize_data(input_list):
    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')

    tokenizer.fit_on_texts(input_list)

    input_seq = tokenizer.texts_to_sequences(input_list)

    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, padding='pre')

    return tokenizer, input_seq

# preprocess input data
tokenizer, input_tensor = tokenize_data(inputs)

def create_categorical_target(targets):
    word={}
    categorical_target=[]
    counter=0
    for trg in targets:
        if trg not in word:
            word[trg]=counter
            counter+=1
        categorical_target.append(word[trg])

    categorical_tensor = tf.keras.utils.to_categorical(categorical_target, num_classes=len(word))
    return categorical_tensor, dict((v,k) for k, v in word.items())

# preprocess output data
target_tensor, trg_index_word = create_categorical_target(targets)

print('input shape: {} and output shape: {}'.format(input_tensor.shape, target_tensor.shape))

# hyperparameters
epochs=50
vocab_size=len(tokenizer.word_index) + 1
embed_dim=512
units=128
target_length=target_tensor.shape[1]

# build RNN Model with tensorflow
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=target_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, dropout=0.2)),
    tf.keras.layers.Dense(units, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(target_length, activation='softmax')
])
model.build(input_shape=(None, target_length))
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4)

# train the model
model.fit(input_tensor, target_tensor, epochs=epochs, callbacks=[early_stop])

def extract_name(sentence):
    sentence = sentence.strip().lower()
    tokens = sentence.split()

    if "i am" in sentence:
        return tokens[tokens.index("am") + 1].capitalize()
    elif "my name is" in sentence:
        return tokens[tokens.index("is") + 1].capitalize()
    elif "this is" in sentence:
        return tokens[tokens.index("is") + 1].capitalize()
    elif "it is" in sentence:
        return tokens[tokens.index("is") + 1].capitalize()
    elif "i'm" in sentence:
        return tokens[tokens.index("i'm") + 1].capitalize()
    elif "im" in tokens:
        return tokens[tokens.index("im") + 1].capitalize()
    return "Human"  # fallback default


def dl_response(sentence):
    sent_seq = []
    doc = nlp(repr(sentence))

    for token in doc:
        if token.text in tokenizer.word_index:
            sent_seq.append(tokenizer.word_index[token.text])
        else:
            sent_seq.append(tokenizer.word_index['<unk>'])

    sent_seq = tf.expand_dims(sent_seq, 0)
    pred = model(sent_seq)
    pred_class = np.argmax(pred.numpy(), axis=1)

    intent = trg_index_word[pred_class[0]]
    res = random.choice(intent_doc[intent])

    # If the intent is one where name is used, try to extract it
    if intent in ["GreetingResponse", "CourtesyGreetingResponse", "CurrentHumanQuery", "WhoAmI"]:
        name = extract_name(sentence)
        res = res.replace("<HUMAN>", name).replace("%%HUMAN%%", name)

    return res, intent

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Create label mapping
intent_labels = list(set(targets))
label_to_id = {label: i for i, label in enumerate(intent_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Vectorize the inputs for ML
vectorizer = TfidfVectorizer()
X_ml = vectorizer.fit_transform(inputs)
y_ml = np.array([label_to_id[label] for label in targets])

# Train Logistic Regression model
ml_model = LogisticRegression(max_iter=1000)
ml_model.fit(X_ml, y_ml)

print("ML Model Accuracy:", ml_model.score(X_ml, y_ml))

def ml_response(sentence):
    vec = vectorizer.transform([sentence])
    pred_class = ml_model.predict(vec)[0]
    intent = id_to_label[pred_class]
    res = random.choice(intent_doc[intent])

    if intent in ["GreetingResponse", "CourtesyGreetingResponse", "CurrentHumanQuery", "WhoAmI"]:
        name = extract_name(sentence)
        res = res.replace("<HUMAN>", name).replace("%%HUMAN%%", name)

    return res, intent

# chat with bot
print("Note: Enter 'quit' to break the loop.")
print("Type 'dl:' to use the deep model or 'ml:' to use the machine learning model.")

while True:
    input_ = input('You: ')
    if input_.lower() == 'quit':
        break

    # choose model based on input prefix
    if input_.lower().startswith("ml:"):
        res, typ = ml_response(input_[3:].strip())
    else:
        res, typ = dl_response(input_)

    print('Bot: {} -- TYPE: {}'.format(res, typ))
    print()

from tensorflow.keras.models import save_model

save_model(model, 'chatbot_dl_model.keras')

import pickle

with open('chatbot_ml_model.pkl', 'wb') as f:
    pickle.dump(ml_model, f)

with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

import pickle

with open('trg_index_word.pkl', 'wb') as f:
    pickle.dump(trg_index_word, f)